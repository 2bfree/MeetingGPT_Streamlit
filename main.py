import re
import streamlit as st

from components.sidebar import sidebar

from ui import (
    wrap_doc_in_html,
    is_query_valid,
    is_file_valid,
    is_open_ai_key_valid,
    display_file_read_error,
)

from core.caching import bootstrap_caching

from core.parsing import read_file
from core.chunking import chunk_file
from core.embedding import embed_files
from core.qa import query_folder
from core.utils import get_llm


EMBEDDING = "openai"
VECTOR_STORE = "faiss"
# MODEL_LIST = ["gpt-3.5-turbo", "gpt-4", "gpt-4-1106-preview"]

# Uncomment to enable debug mode
# MODEL_LIST.insert(0, "debug")

st.set_page_config(page_title="SFA KnowledgeGPT - beta", page_icon="ğŸ“–", layout="wide")
st.header("SFA KnowledgeGPT - beta")

# Enable caching for expensive functions
bootstrap_caching()

sidebar()

openai_api_key = st.session_state.get("OPENAI_API_KEY")


# if not openai_api_key:
#     st.warning(
#         "OpenAI API keyê°€ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
#     )


uploaded_file = st.file_uploader(
    "ê²€ìƒ‰í•  íŒŒì¼ì„ ì—…ë¡œë“œ í•˜ì„¸ìš”. (í˜„ì¬ëŠ” pdf, docx, txtë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤.)",
    type=["pdf", "docx", "txt"],
    # help="ìŠ¤ìº”í•œ ë¬¸ì„œ ë¶„ì„ì€ ì•„ì§ ëª»í•©ë‹ˆë‹¤.",
)

# model: str = st.selectbox("GPT ëª¨ë¸ ì„ íƒ", options=MODEL_LIST)  # type: ignore

selected_model = st.session_state.get("selected_model","default_model")

# with st.expander("ì„¸ë¶€ ì˜µì…˜ ë³´ê¸°"):
#     return_all_chunks = st.checkbox("ë²¡í„° ê²€ìƒ‰ì—ì„œ ê²€ìƒ‰ëœ ëª¨ë“  ì¡°ê°(chunks) ë³´ê¸°")
#     show_full_doc = st.checkbox("ì—…ë¡œë“œí•œ íŒŒì¼ ë‚´ìš© ë³´ê¸°")

# ì„¸ë¶€ ì˜µì…˜ ê°’ ë¶ˆëŸ¬ì˜¤ê¸°
return_all_chunks = st.session_state.get("return_all_chunks", False)
show_full_doc = st.session_state.get("show_full_doc", False)

if not uploaded_file:
    st.stop()

try:
    file = read_file(uploaded_file)
except Exception as e:
    display_file_read_error(e, file_name=uploaded_file.name)

chunked_file = chunk_file(file, chunk_size=200, chunk_overlap=20) #overlapì€ chunk sizeì˜ 10%ì— ë§ì¶¤

if not is_file_valid(file):
    st.stop()

if not is_open_ai_key_valid(openai_api_key, selected_model):
    st.stop()


with st.spinner("ë¬¸ì„œë¥¼ ì¸ë±ì‹±í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì‹œê°„ì´ ì¡°ê¸ˆ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤."):
    folder_index = embed_files(
        files=[chunked_file],
        embedding=EMBEDDING if selected_model != "debug" else "debug",
        vector_store=VECTOR_STORE if selected_model != "debug" else "debug",
        openai_api_key=openai_api_key,
    )

with st.form(key="qa_form"):
    query = st.text_area("ì—…ë¡œë“œí•œ ë¬¸ì„œì— ëŒ€í•´ ë¬¼ì–´ë³´ì„¸ìš”.")
    submit = st.form_submit_button("ì§ˆë¬¸í•˜ê¸°")


if show_full_doc:
    with st.expander("ë¬¸ì„œ ìì„¸íˆ ë³´ê¸°"):
        # Hack to get around st.markdown rendering LaTeX
        st.markdown(f"<p>{wrap_doc_in_html(file.docs)}</p>", unsafe_allow_html=True)


if submit:
    if not is_query_valid(query):
        st.stop()

    # Output Columns
    # answer_col, sources_col = st.columns(2)

    llm = get_llm(model=selected_model, openai_api_key=openai_api_key, temperature=0.1)
    result = query_folder(
        folder_index=folder_index,
        query=query,
        return_all=return_all_chunks,
        llm=llm,
    )


    # ë¬¸ì¥ìœ¼ë¡œ ë‚˜ëˆ„ê¸°
    sentences_answer = re.split('(?<=\.)\s', result.answer)

    # with answer_col:
    st.markdown("#### ê²°ê³¼")
    st.markdown(result.answer)
    # for sentences_answer in sentences_answer :
    #     st.markdown(sentences_answer)

    # with sources_col:
    st.markdown("#### ì¶œì²˜")
    for source in result.sources:
        st.markdown(source.page_content)
        st.markdown(source.metadata["source"])
        st.markdown("---")